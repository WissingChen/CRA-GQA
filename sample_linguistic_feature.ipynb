{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample the semantic structure graph feature for linguistic causal back-door intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we process the QA pairs using the Stanza toolkit.\n",
    "- Then, we encode them with CLIP and perform sampling using the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deconstructing QA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza as snlp\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/star/train.csv\") # csv file of the train set\n",
    "df = df[[\"question\", \"answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init StanfordNLP pipeline\n",
    "# snlp.download('en')\n",
    "nlp = snlp.Pipeline()\n",
    "data = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StanfordNLP for text analysis and map construction\n",
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    processed_data = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            # token = sentence.tokens[word.index - 1]\n",
    "            processed_data.append({\n",
    "                'text': word.text,\n",
    "                'lemma': word.lemma,\n",
    "                'upos': word.upos,\n",
    "                'xpos': word.xpos,\n",
    "                'head': word.head,  # head index (1-based)\n",
    "                'deprel': word.deprel\n",
    "            })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def extract_components(question_data, answer_data):\n",
    "    # Question type: Usually a question word\n",
    "    question_type = next((word['text'] for word in question_data if word['upos'] in ['PRON', 'ADV', 'DET']), None)\n",
    "\n",
    "    # Subject in question: 'nsubj' (noun subject) or 'nsubjpass' (passive noun subject) in dependency\n",
    "    subject = next((word['text'] for word in question_data if word['deprel'] in ['nsubj', 'nsubjpass']), None)\n",
    "    \n",
    "    # All verbs in question\n",
    "    verbs = list(set(word['text'] for word in question_data if word['upos'] == 'VERB'))\n",
    "\n",
    "    # Object in question: all entities except the subject\n",
    "    objs = list(set(word['text'] for word in question_data if word['deprel'] not in ['nsubj', 'nsubjpass'] and word['upos'] in ['NOUN', 'PROPN', 'PROPN', 'ADJ']))\n",
    "    \n",
    "    # Answers set\n",
    "    answer = ' '.join([word['text'] for word in answer_data])\n",
    "\n",
    "    # a_subject = next((word['text'] for word in answer_data if word['deprel'] in ['nsubj', 'nsubjpass']), None)\n",
    "    \n",
    "    # All verbs in the answer\n",
    "    a_verbs = list(set(word['text'] for word in answer_data if word['upos'] == 'VERB'))\n",
    "\n",
    "    # Object in the answer: all entities except the subject\n",
    "    a_objs = list(set(word['text'] for word in answer_data if word['deprel'] not in ['nsubj', 'nsubjpass'] and word['upos'] in ['NOUN', 'PROPN']))\n",
    "    \n",
    "    components = {\n",
    "        'question': ' '.join([word['text'] for word in question_data]),\n",
    "        'question_type': question_type,\n",
    "        'subject': subject,\n",
    "        'verbs': verbs,\n",
    "        'objects': objs,\n",
    "        'answer': answer,\n",
    "        'answer_verbs': a_verbs,\n",
    "        'answer_objects': a_objs,\n",
    "    }\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_components = []\n",
    "for i in range(len(data)):\n",
    "    question_data = process_text(data[i]['question'])\n",
    "    answer_data = process_text(data[i]['answer'])\n",
    "    components = extract_components(question_data, answer_data)\n",
    "    data_components.append(components)\n",
    "    print(f\"\\rProgress: {i+1}/{len(data)}\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result as a JSON file\n",
    "with open('data/star/causal_feature/data_components.json', 'w') as f:\n",
    "    json.dump(data_components, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. encoding the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_with_clip(text, model, processor):\n",
    "    inputs = processor(text=text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "    return text_features\n",
    "\n",
    "def encode_components(components, model, processor):\n",
    "    encoded_components = {}\n",
    "    for key, value in components.items():\n",
    "        if isinstance(value, list):\n",
    "            # Encode list of texts\n",
    "            text = ' '.join(value)\n",
    "        else:\n",
    "            # Encode single text\n",
    "            text = value if value else ''\n",
    "        encoded_components[key] = encode_text_with_clip(text, model, processor)\n",
    "    return encoded_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "encoded_data_components = []\n",
    "for i in range(len(data)):\n",
    "    components = data_components[i]\n",
    "    encoded_components = encode_components(components, clip_model, clip_processor)\n",
    "    encoded_data_components.append(encoded_components)\n",
    "    print(f\"\\rProgress: {i+1}/{len(data)}\", end='')\n",
    "# 将结果保存为npy文件\n",
    "torch.save(encoded_data_components, 'data/star/causal_feature/encoded_data_components.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constructing the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def build_knowledge_graph(encoded_components):\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    # Define node features\n",
    "    nodes.append(encoded_components['question'])\n",
    "    nodes.append(encoded_components['question_type'])\n",
    "    nodes.append(encoded_components['subject'])\n",
    "    nodes.append(encoded_components['verbs'])\n",
    "    nodes.append(encoded_components['objects'])\n",
    "    nodes.append(encoded_components['answer'])\n",
    "    nodes.append(encoded_components['answer_verbs'])\n",
    "    nodes.append(encoded_components['answer_objects'])\n",
    "\n",
    "    # Define edges\n",
    "    edge_index = []\n",
    "\n",
    "    # Add edges according to the specified relationships\n",
    "    edge_index.extend([\n",
    "        (0, 5),  # q--a\n",
    "        (0, 1),  # q--q_type\n",
    "        (0, 2),  # q--q_sub\n",
    "        (0, 3),  # q--q_verb\n",
    "        (0, 4),  # q--q_obj\n",
    "        (2, 4),  # q_sub--q_obj\n",
    "        (2, 3),  # q_sub--q_verb\n",
    "        (4, 3),  # q_obj--q_verb\n",
    "        (5, 6),  # a--a_verb\n",
    "        (5, 7),  # a--a_obj\n",
    "        (6, 7),  # a_verb--a_obj\n",
    "        (2, 5)   # q_sub--a\n",
    "    ])\n",
    "\n",
    "    # Convert edge_index to tensor\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Convert nodes to tensor\n",
    "    x = torch.cat(nodes, dim=0)\n",
    "\n",
    "    # Create PyTorch Geometric Data object\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "for i in range(len(encoded_data_components)):\n",
    "    encoded_components = encoded_data_components[i]\n",
    "    knowledge_graph = build_knowledge_graph(encoded_components)\n",
    "    graphs.append(knowledge_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "\n",
    "def kmean(x, k=512):\n",
    "    x = torch.tensor(x)\n",
    "    x = x.numpy()\n",
    "    x = x.reshape([-1, 768*8])\n",
    "    # Apply K-means algorithm\n",
    "    print(\"feature sample:\", x.shape[0])\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, random_state=43, verbose=True).fit(x)\n",
    "    print(\"clustering done\")\n",
    "    # Get the cluster center point\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    print(\"Get centers\")\n",
    "    # Gets the cluster label for each point\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Initializes a list to store the member characteristics of each cluster\n",
    "    cluster_features = [x[labels == i] for i in range(k)]\n",
    "    # Calculate the feature mean of each cluster\n",
    "    cluster_means = [np.mean(cluster, axis=0) for cluster in cluster_features]\n",
    "    print(\"Get mean\")\n",
    "    return cluster_centers, cluster_means\n",
    "    \n",
    "def cluster_knowledge_graphs(graphs, n_clusters=512, batch_size=100):\n",
    "    # Flatten all node features into a single matrix\n",
    "    all_features = []\n",
    "    for graph in graphs:\n",
    "        all_features.append(graph.x.reshape(1, -1))\n",
    "\n",
    "    all_features = torch.cat(all_features, dim=0).numpy()\n",
    "\n",
    "    # Perform MiniBatchKMeans clustering\n",
    "    cluster_centers, cluster_means = kmean(all_features)\n",
    "    cluster_centers = torch.from_numpy(cluster_centers).reshape(-1, 8, 768)\n",
    "    cluster_means = torch.tensor(cluster_means).reshape(-1, 8, 768)\n",
    "    # Reorganize graphs based on clustering results\n",
    "    # Define edges\n",
    "    edge_index = []\n",
    "\n",
    "    # Add edges according to the specified relationships\n",
    "    edge_index.extend([\n",
    "        (0, 5),  # q--a\n",
    "        (0, 1),  # q--q_type\n",
    "        (0, 2),  # q--q_sub\n",
    "        (0, 3),  # q--q_verb\n",
    "        (0, 4),  # q--q_obj\n",
    "        (2, 4),  # q_sub--q_obj\n",
    "        (2, 3),  # q_sub--q_verb\n",
    "        (4, 3),  # q_obj--q_verb\n",
    "        (5, 6),  # a--a_verb\n",
    "        (5, 7),  # a--a_obj\n",
    "        (6, 7),  # a_verb--a_obj\n",
    "        (2, 5)   # q_sub--a\n",
    "    ])\n",
    "\n",
    "    # Convert edge_index to tensor\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Create PyTorch Geometric Data object\n",
    "    clustered_centers_graphs = Data(x=cluster_centers, edge_index=edge_index)\n",
    "    clustered_means_graphs = Data(x=cluster_means, edge_index=edge_index)\n",
    "    return clustered_centers_graphs, clustered_means_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering knowledge graph\n",
    "clustered_centers_graphs, clustered_means_graphs = cluster_knowledge_graphs(graphs, n_clusters=512, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"k_center\": clustered_centers_graphs, \"k_mean\": clustered_means_graphs}, \"data/star/causal_feature/qa_graphs.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scannet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
